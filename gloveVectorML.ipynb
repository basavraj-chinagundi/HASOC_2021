{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gloveVectorML.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMZ-Sk1VSZ3o",
        "outputId": "591c0e56-84c2-4852-a7f5-8e1900cf5fb5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kd6VJkHJssk"
      },
      "source": [
        "import pandas as pd\n",
        "train=pd.read_csv('/content/drive/MyDrive/Workspace/hasocpaper/trainset.csv')\n",
        "test=pd.read_csv('/content/drive/MyDrive/Workspace/hasocpaper/testset.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fKoZZIkES4m"
      },
      "source": [
        "train.dropna(inplace=True)\n",
        "test.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhYDD2fCSg7c"
      },
      "source": [
        "TRAINING AND TESTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBIGSkWaSMJU"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "import pickle\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "#Import module to split the datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Import modules to evaluate the metrics\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,roc_auc_score,roc_curve,auc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wrZ1IV9oBzF"
      },
      "source": [
        "root_folder='/content/drive/MyDrive/Workspace/Alta/'\n",
        "data_folder_name='dataset'\n",
        "glove_filename='glove.840B.300d.txt'\n",
        "\n",
        "# Variable for data directory\n",
        "DATA_PATH = os.path.abspath(os.path.join(root_folder, data_folder_name))\n",
        "glove_path = os.path.abspath(os.path.join(DATA_PATH, glove_filename))\n",
        "\n",
        "# # Both train and test set are in the root data directory\n",
        "# train_path = DATA_PATH\n",
        "# test_path = DATA_PATH\n",
        "\n",
        "# #Relevant columns\n",
        "# TEXT_COLUMN = 'features'\n",
        "# TARGET_COLUMN = 'label'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xaNqGjrTspd",
        "outputId": "fcdadb65-d7f1-4c53-f4c4-8298ab77b2a0"
      },
      "source": [
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "#glove_input_file = glove_filename\n",
        "word2vec_output_file = glove_filename+'.word2vec'\n",
        "glove2word2vec('/content/drive/MyDrive/Workspace/Alta/glove.840B.300d.txt', word2vec_output_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2196017, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFMl4aNnVNiL"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "# load the Stanford GloVe model\n",
        "word2vec_output_file = glove_filename+'.word2vec'\n",
        "model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnijCG68Tvwt"
      },
      "source": [
        "class Word2VecVectorizer:\n",
        "  def __init__(self, model):\n",
        "    print(\"Loading in word vectors...\")\n",
        "    self.word_vectors = model\n",
        "    print(\"Finished loading in word vectors\")\n",
        "\n",
        "  def fit(self, data):\n",
        "    pass\n",
        "\n",
        "  def transform(self, data):\n",
        "    # determine the dimensionality of vectors\n",
        "    v = self.word_vectors.get_vector('king')\n",
        "    self.D = v.shape[0]\n",
        "\n",
        "    X = np.zeros((len(data), self.D))\n",
        "    n = 0\n",
        "    emptycount = 0\n",
        "    for sentence in data:\n",
        "      tokens = sentence.split()\n",
        "      vecs = []\n",
        "      m = 0\n",
        "      for word in tokens:\n",
        "        try:\n",
        "          # throws KeyError if word not found\n",
        "          vec = self.word_vectors.get_vector(word)\n",
        "          vecs.append(vec)\n",
        "          m += 1\n",
        "        except KeyError:\n",
        "          pass\n",
        "      if len(vecs) > 0:\n",
        "        vecs = np.array(vecs)\n",
        "        X[n] = vecs.mean(axis=0)\n",
        "      else:\n",
        "        emptycount += 1\n",
        "      n += 1\n",
        "    print(\"Numer of samples with no words found: %s / %s\" % (emptycount, len(data)))\n",
        "    return X\n",
        "\n",
        "\n",
        "  def fit_transform(self, data):\n",
        "    self.fit(data)\n",
        "    return self.transform(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UjwbPo-UNTE"
      },
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/Workspace/hasoc/trainpSet.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "YUPKDe6kXe66",
        "outputId": "90640c32-2595-4a96-8989-4ecf56acbf2f"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>features</th>\n",
              "      <th>task_1</th>\n",
              "      <th>task_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>wealth if you made it through this amp were no...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>PRFN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Technically that  still turning back the clock...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>OFFN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>VMBJP BJP Bengal India narendramodi JPNadda Am...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>krtoprak yigit Soldier of Japan Who has dick head</td>\n",
              "      <td>HOF</td>\n",
              "      <td>OFFN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>blueheartedly You  be better off asking who DO...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>OFFN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3838</th>\n",
              "      <td>BBCNews Let the dog deal with wanker once he  ...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>PRFN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3839</th>\n",
              "      <td>India has suffered  lot That Chinese bastard s...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>HATE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3840</th>\n",
              "      <td>People didn  give seats majority to BJP see Be...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>HATE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3841</th>\n",
              "      <td>KanganaTeam This is such  vile xenophobic and ...</td>\n",
              "      <td>HOF</td>\n",
              "      <td>PRFN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3842</th>\n",
              "      <td>iPpgStmILw SI ChinaDaily ChineseVirus WuhanVir...</td>\n",
              "      <td>NOT</td>\n",
              "      <td>NONE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3843 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               features task_1 task_2\n",
              "0     wealth if you made it through this amp were no...    HOF   PRFN\n",
              "1     Technically that  still turning back the clock...    HOF   OFFN\n",
              "2     VMBJP BJP Bengal India narendramodi JPNadda Am...    NOT   NONE\n",
              "3     krtoprak yigit Soldier of Japan Who has dick head    HOF   OFFN\n",
              "4     blueheartedly You  be better off asking who DO...    HOF   OFFN\n",
              "...                                                 ...    ...    ...\n",
              "3838  BBCNews Let the dog deal with wanker once he  ...    HOF   PRFN\n",
              "3839  India has suffered  lot That Chinese bastard s...    HOF   HATE\n",
              "3840  People didn  give seats majority to BJP see Be...    HOF   HATE\n",
              "3841  KanganaTeam This is such  vile xenophobic and ...    HOF   PRFN\n",
              "3842  iPpgStmILw SI ChinaDaily ChineseVirus WuhanVir...    NOT   NONE\n",
              "\n",
              "[3843 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_CqrgeJUzH6"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "  \n",
        "# label_encoder object knows how to understand word labels.\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "  \n",
        "# Encode labels in column 'species'.\n",
        "df['new_label1']= label_encoder.fit_transform(df['task_1'].astype(str))\n",
        "# df['new_label2']= label_encoder.fit_transform(df['task_2'].astype(str))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55sXmwB0USJy"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train = train.tweet.values\n",
        "y_1 = train.label.values\n",
        "# y_2 = df.new_label2.values\n",
        "\n",
        "# X_train, X_val, y_train, y_val =\\\n",
        "#     train_test_split(X, y, test_size=0.2, random_state=2020)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng45SG4CU_eQ"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
        "from sklearn.svm import SVC,LinearSVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier,RandomForestClassifier,GradientBoostingClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkZ16aWkCz0X",
        "outputId": "a1a6fa99-d4f9-44bd-81f7-2036c92525e9"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([\"as a woman you shouldn't complain about cleaning up your house. amp as a man you should always take the trash out...\",\n",
              "       'boy dats cold...tyga dwn bad for cuffin dat hoe in the 1st place',\n",
              "       'dawg you ever fuck a bitch and she start to cry? you be confused as shit',\n",
              "       ...,\n",
              "       \"people didn't give 300 seats majority to bjp to see bengal burning ... if they can't fix this, they hv no right to continue in office... don't take our votes for granted ...\",\n",
              "       'this is such a vile, xenophobic and uneducated comment... i m struggling to believe someone thinks like this, let alone posted this? daylight islamophobia and it should be stopped. elections',\n",
              "       'chinese virus wuhan virus is the correct name for the pandemic . shameless'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCfrjiaTDtTR"
      },
      "source": [
        "for sentence in X_train:\n",
        "    # print(type(sentence))\n",
        "    if(type(sentence)!=str):\n",
        "        print(sentence)\n",
        "    #   tokens = sentence.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "051n9blSVEV7",
        "outputId": "d2f5c284-a351-4940-f8fe-f8a937db6d22"
      },
      "source": [
        "vectorizer = Word2VecVectorizer(model)\n",
        "# Get the sentence embeddings for the train dataset\n",
        "Xtrain = vectorizer.fit_transform(X_train)\n",
        "Ytrain1 = y_1\n",
        "# Ytrain2=y_2\n",
        "# Get the sentence embeddings for the test dataset\n",
        "# Xtest = vectorizer.transform(X_val)\n",
        "# Ytest = y_val"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading in word vectors...\n",
            "Finished loading in word vectors\n",
            "Numer of samples with no words found: 161 / 76578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9QNioNTVGd7",
        "outputId": "4852120b-e678-4cb9-9694-0f9fe169610c"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "# # create the model, train it, print scores\n",
        "clf1 = GaussianNB().fit(Xtrain, Ytrain1)\n",
        "# clf2 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(Xtrain, Ytrain)\n",
        "clf3 = LogisticRegression(random_state=0).fit(Xtrain, Ytrain1)\n",
        "clf4 = MLPClassifier(random_state=1, max_iter=300).fit(Xtrain, Ytrain1)\n",
        "# clf5 = make_pipeline(StandardScaler(), SVC(gamma='auto')).fit(Xtrain, Ytrain1)\n",
        "clf6 = VotingClassifier(estimators=[ ('nb', clf1),('lr', clf3),('mlp',clf4)], voting='hard').fit(Xtrain, Ytrain1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjnR_9vqVhD2"
      },
      "source": [
        "test=pd.read_csv('/content/drive/MyDrive/Workspace/hasoc/testpSet.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "JRYN1KDdulU_",
        "outputId": "a3c58933-ac2f-41e0-8d9c-f0ba6b32dfe3"
      },
      "source": [
        "test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_id</th>\n",
              "      <th>features</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>60c5d6bf5659ea5e55deffcb</td>\n",
              "      <td>Fewer people coming in for vaccinations So sad...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>60c5d6bf5659ea5e55df028c</td>\n",
              "      <td>MattHancock This may all be true But What did ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>60c5d6bf5659ea5e55def377</td>\n",
              "      <td>Layla EFC  ve unfollowed him the wanker</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>60c5d6bf5659ea5e55def4c7</td>\n",
              "      <td>You guys are losing it all over the world The ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60c5d6bf5659ea5e55df01a6</td>\n",
              "      <td>And thus death laughs It is sad merriment But ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1276</th>\n",
              "      <td>60c5d6bf5659ea5e55def74c</td>\n",
              "      <td>AJEnglish Shameless Hinduphobia and bigotry ca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1277</th>\n",
              "      <td>60c5d6bf5659ea5e55df0380</td>\n",
              "      <td>DonVocero Real Motherfucker</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1278</th>\n",
              "      <td>60c5d6bf5659ea5e55defaeb</td>\n",
              "      <td>Gunna was in cunt mode on DS</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1279</th>\n",
              "      <td>60c5d6bf5659ea5e55df03c3</td>\n",
              "      <td>roomorgue  adore all my hooker friends includi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1280</th>\n",
              "      <td>60c5d6bf5659ea5e55def60b</td>\n",
              "      <td>DorisVulva People everywhere gave  fuck for Do...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1281 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                           _id                                           features\n",
              "0     60c5d6bf5659ea5e55deffcb  Fewer people coming in for vaccinations So sad...\n",
              "1     60c5d6bf5659ea5e55df028c  MattHancock This may all be true But What did ...\n",
              "2     60c5d6bf5659ea5e55def377            Layla EFC  ve unfollowed him the wanker\n",
              "3     60c5d6bf5659ea5e55def4c7  You guys are losing it all over the world The ...\n",
              "4     60c5d6bf5659ea5e55df01a6  And thus death laughs It is sad merriment But ...\n",
              "...                        ...                                                ...\n",
              "1276  60c5d6bf5659ea5e55def74c  AJEnglish Shameless Hinduphobia and bigotry ca...\n",
              "1277  60c5d6bf5659ea5e55df0380                        DonVocero Real Motherfucker\n",
              "1278  60c5d6bf5659ea5e55defaeb                       Gunna was in cunt mode on DS\n",
              "1279  60c5d6bf5659ea5e55df03c3  roomorgue  adore all my hooker friends includi...\n",
              "1280  60c5d6bf5659ea5e55def60b  DorisVulva People everywhere gave  fuck for Do...\n",
              "\n",
              "[1281 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AveJQs866mi3"
      },
      "source": [
        "X_test = test.tweet.values\n",
        "# y_test = test.new_label.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-SS1YQB9sih",
        "outputId": "78eb228c-a1a2-4a8d-82e6-359cedd3d063"
      },
      "source": [
        "Xtest = vectorizer.fit_transform(X_test)\n",
        "# Ytest = y_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numer of samples with no words found: 1 / 1280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uef5MgI69g8U"
      },
      "source": [
        "y_pred=clf6.predict(Xtest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1Dmj9y_ZatU",
        "outputId": "17b1fd84-4f25-471c-b44b-19545fb0952a"
      },
      "source": [
        "y_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, ..., 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnlKXHSI93rm"
      },
      "source": [
        "predictions=label_encoder.inverse_transform(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t79MIuU--NRR"
      },
      "source": [
        "p=pd.DataFrame(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtZ6jpZd-dV0",
        "outputId": "31643906-07eb-43aa-df6f-c4fb72707d4e"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(test.label, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.70      0.65       482\n",
            "           1       0.80      0.73      0.77       798\n",
            "\n",
            "    accuracy                           0.72      1280\n",
            "   macro avg       0.71      0.72      0.71      1280\n",
            "weighted avg       0.73      0.72      0.72      1280\n",
            "\n"
          ]
        }
      ]
    }
  ]
}